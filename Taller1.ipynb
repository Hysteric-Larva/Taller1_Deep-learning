{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Challenge: Detección de elementos patrimoniales\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1661833329a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdamax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;31m# FIXME: why have numpy.lib if everything is imported here??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/numpy/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marraysetops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnpyio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfinancial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrayterator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD,RMSprop,Adamax,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Conv2D, MaxPooling2D,LeakyReLU,GaussianDropout,GlobalMaxPooling2D,SpatialDropout2D\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from keras.constraints import max_norm\n",
    "from contextlib import redirect_stdout\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Funciones a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*image_set_generator* Es una función que toma cada imagen, la transforma en un arreglo tri-dimensional y luego la almacena en una lista que posteriormente es transformada en arreglo para su posterior entrenamiento o prediccion según seal el caso.\n",
    "\n",
    "Parámetros:\n",
    "\n",
    "X: Corresponde al arreglo que contenga los nombres de las imagenes a utilizar.\n",
    "image_path: Corresponde a un string que contiene la direccion de donde se encuentran (carpeta)\n",
    "image_format: La extensión del archivo, se añadio para poder recibir más tipos de imagenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_set_generator(X,image_path,image_format):\n",
    "    gen = []\n",
    "    for i in X:\n",
    "        image =  tf.keras.preprocessing.image.load_img(image_path+i+'.'+image_format,target_size=None)\n",
    "        data =keras.preprocessing.image.img_to_array(image)\n",
    "        gen.append(data)\n",
    "       \n",
    "    return np.asarray(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat_encode/decode: Este par de funciones transforma el arreglo de etiquetas en categorias binarias para mejorar así su representación y evitar problemas de entrenamiento del modelo. \n",
    "\n",
    "Parámetros:\n",
    "\n",
    "Data: informacion a transformar o revertir transformacion (lista o arreglo)\n",
    "cat: conjunto de categorias (lista o arreglo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode(data,cat):\n",
    "    data=list(data)\n",
    "    cat=list(cat)\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(cat)):\n",
    "            if data[i]==cat[j]:\n",
    "                data[i]=j\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "    return keras.utils.to_categorical(data,len(cat))\n",
    "    #return np.asarray(data)\n",
    "def cat_decode(data,cat):\n",
    "    #Y=data\n",
    "    Y= [np.argmax(y, axis=None, out=None) for y in data]\n",
    "    cat=list(cat)\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        Y[i]=cat[Y[i]]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autopredict automatiza el proceso de prediccion y guardado en un archivo csv. Además incorpora una opcion para mostrar resultados\n",
    "\n",
    "auto_predict(data_x,data_y,cat,example_show):\n",
    "\n",
    "data_x: corresponde al conjunto de datos que se busca predecir (X_train)\n",
    "data_set: corresponde al conjunto de datos que se ingresará en el archivo csv\n",
    "cat:categorias\n",
    "example_show: True/False si se quiere adjuntar un muestreo de los primeros 20 resultados de la prediccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_predict(data_x,data_set,cat,example_show):\n",
    "    pred = model.predict(data_x)\n",
    "    y_test = cat_decode(pred,cat)\n",
    "    data_set['Expected']= y_test\n",
    "    data_set.to_csv('sample_submission_1234.csv',columns=['Id','Expected'],index=False)\n",
    "    if example_show==True:\n",
    "        return print(\"Exito!\\n\",y_test[0:20])\n",
    "    else:\n",
    "        return print(\"Exito\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargando la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('./data/train_labels.csv')\n",
    "test_labels = pd.read_csv('./data/sample_submission.csv')\n",
    "train_path = './data/train_images/'\n",
    "test_path = './data/test_images/'\n",
    "X_names = train_labels[\"Id\"].values\n",
    "X_train = image_set_generator(X_names,train_path,'jpg')\n",
    "\n",
    "\n",
    "Y_train = train_labels[\"Expected\"]\n",
    "cat = Y_train.unique()\n",
    "Y_train=cat_encode(Y_train,cat)\n",
    "Y_train = np.asarray(Y_train)\n",
    "X_test_names = test_labels[\"Id\"].values\n",
    "X_test  = image_set_generator(X_test_names,test_path,'jpg')\n",
    "\n",
    "#X_train =  preprocessing.scale(X_train)\n",
    "#X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comentar solo en caso de que se prueben modelos 1 y 2\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.3)\n",
    "datagen.fit(X_train)\n",
    "train_generator = datagen.flow(\n",
    "    X_train,Y_train,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    sample_weight=None,\n",
    "    seed=None,\n",
    "    save_to_dir=None,\n",
    "    save_prefix=\"\",\n",
    "    save_format=\"png\",\n",
    "    subset=\"training\",\n",
    ")\n",
    "val_generator= datagen.flow(\n",
    "    X_train,Y_train,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    sample_weight=None,\n",
    "    seed=None,\n",
    "    save_to_dir=None,\n",
    "    save_prefix=\"\",\n",
    "    save_format=\"png\",\n",
    "    subset=\"validation\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####                              ENCODER TEST                         ####\n",
    "\n",
    "#encoder/decoder, quitar comentarios solo en caso de prueba. SPOILER: FUNCIONAN!\n",
    "#print(Y_train)\n",
    "#Y_train=cat_encode(Y_train,cat)\n",
    "#Y_train=cat_decode(Y_train,cat)\n",
    "#print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando el modelo (1/x RELUciente) (1,2 y 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65% Sin data augmentation, usando SGD ( con la configuracion mostrada abajo)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,(2,2), padding='same', input_shape=(128,128,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, (2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (2,2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (2,2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "85.75% (Sin data augmentation, ADA) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(128,128,3) ))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(32,(2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(64, (2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(256, (2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(512, (2,2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(GaussianDropout(rate = 0.2))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 0.912775993347168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(128,128,3) ))\n",
    "#model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "model.add(Conv2D(4,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(SpatialDropout2D(0.2))\n",
    "\n",
    "\n",
    "\n",
    "#model.add(SpatialDropout2D(0.3))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.02))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(SpatialDropout2D(0.4))\n",
    "\n",
    "#model.add(Conv2D(256,(5,5), padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(LeakyReLU(alpha=0.03))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(SpatialDropout2D(0.45))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(SpatialDropout2D(0.5))\n",
    "model.add(Flatten())\n",
    "#model.add(GaussianDropout(rate = 0.2))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "# Plot the model\n",
    "plot_model(model, to_file='model.png')\n",
    "\n",
    "# Display the image\n",
    "data = plt.imread('model.png')\n",
    "plt.imshow(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks y otras configuraciones para compiler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########################################        OLD                    ####################################\n",
    "Nota: solo transformar a codigo si se quiere probar los primeros dos modelos\n",
    "\n",
    "#########################################           CALLBACKS           #########################################\n",
    "callback1 = EarlyStopping(monitor='val_categorical_accuracy', patience=5)\n",
    "path_checkpoint = '/tmp/checkpoint'\n",
    "callback2 = ModelCheckpoint(\n",
    "    filepath=path_checkpoint,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "callback3 = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=7, mode='auto')\n",
    "#########################################           OPTIMIZER           #########################################\n",
    "#opt = SGD(learning_rate=0.5, momentum=0.15, nesterov=True, name='SGD')\n",
    "#opt = Ftrl(learning_rate=0.001,learning_rate_power=-0.5,initial_accumulator_value=0.1,l1_regularization_strength=0.1,\n",
    "#    l2_regularization_strength=0.1,name=\"Ftrl\",l2_shrinkage_regularization_strength=0.001)\n",
    "#opt  =RMSprop()\n",
    "opt=Adam(amsgrad=True)\n",
    "########################################           LOSS FUCTION           #######################################\n",
    "\n",
    "#########################################           Metrics           #########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################        NUEVO                    #######################################\n",
    "#Nota: Para el tercer modelo (test) con data augmentation\n",
    "    \n",
    "    \n",
    "#########################################           CALLBACKS           #########################################\n",
    "early_stop = EarlyStopping(monitor='val_categorical_accuracy', patience=12)\n",
    "\n",
    "check_point = ModelCheckpoint(\n",
    "    filepath='/tmp/checkpoint',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.85, patience=7, verbose=0, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0.00006\n",
    ")\n",
    "\n",
    "#########################################           OPTIMIZER           #########################################\n",
    "opt=Adam(amsgrad=True)\n",
    "########################################           LOSS FUCTION           #######################################\n",
    "\n",
    "#########################################           Metrics           #########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilado y entrenamiento de nuestro modelo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Para el modelo 1 y 2\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"categorical_accuracy\",'accuracy'])\n",
    "train=model.fit(datagen[0],datagen[1],epochs=100,validation_split=0.3,batch_size=128,shuffle=True,callbacks=[callback1,callback2,callback3])\n",
    "model.load_weights(path_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model.fit(train_generator,epochs=120,validation_data= val_generator,callbacks=[early_stop,check_point,reduce_lr])\n",
    "model.load_weights('/tmp/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=train.history\n",
    "plt.plot(history['categorical_accuracy'],'r',label = 'categorical_accuracy')\n",
    "\n",
    "# Plot the validation loss\n",
    "plt.plot(history['val_categorical_accuracy'],label = 'val_categorical_accuracy')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train.history)\n",
    "train_data['epoch'] = train.epoch\n",
    "train_data.to_excel('train_data_model3.xlsx',index=False)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_categorical_accuracy = max(list(train_data['val_categorical_accuracy']))\n",
    "val_categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_predict(X_test,test_labels,cat,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 4:  MatNetV6 (Mejor en KAGGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 35\n",
    "r = 2.2\n",
    "N = n\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Conv2D(n, (2, 2),input_shape=(128,128,3), activation='relu', padding='same')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "for i in range(5):\n",
    "    if i < 4: n = round(r*n)       \n",
    "    model.add(Conv2D(n, (2, 2), activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(80,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy','categorical_accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_categorical_accuracy', patience=12)\n",
    "\n",
    "check_point = ModelCheckpoint(\n",
    "    filepath='/tmp/checkpoint',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_categorical_accuracy', factor=0.4, patience=7, verbose=0, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "train = model.fit(X_train,Y_train,epochs=120,validation_split=0.3,batch_size=64,shuffle=True,callbacks=[early_stop,check_point,reduce_lr])\n",
    "model.load_weights('/tmp/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train.history)\n",
    "train_data['epoch'] = train.epoch\n",
    "train_data.to_excel('train_data.xlsx',index=False)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_categorical_accuracy = max(list(train_data['val_categorical_accuracy']))\n",
    "val_categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(train_data): \n",
    "    sns.set_style('darkgrid')\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(train_data['accuracy']) \n",
    "    axs[0].plot(train_data['val_accuracy']) \n",
    "    axs[0].set_title('Model Categorical Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy') \n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(train_data['loss']) \n",
    "    axs[1].plot(train_data['val_loss']) \n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss') \n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_model_history(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "y_pred = cat_decode(pred,cat)\n",
    "\n",
    "test_labels['Expected']= y_pred\n",
    "test_labels.to_csv('sample_submission_mateneuronas.csv',columns=['Id','Expected'],index=False)\n",
    "test_labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrencias en predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.groupby('Expected').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelo\n",
    "model.save('model.h5')\n",
    "# Summary\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Información del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"model_info.txt\",\"w\") \n",
    "texto = f\"\"\"\\\n",
    "MODELO\n",
    "- 6 capas convolucionales\n",
    "- Arquitectura tipo cono, n = {N}, r = {r}\n",
    "- 1 capas densa de 80 neuronas\n",
    "- Activaciones Relu\n",
    "- Batch Normalization\n",
    "- Dropout = 0.2\n",
    "- Parámetros = {model.count_params()}\n",
    "\n",
    "ENTRENAMIENTO\n",
    "- Optimizador Adam\n",
    "- Checkpoint: monitor = val_categorical_accuracy\n",
    "- Early stopping: monitor = val_categorical_accuracy, patience = 12\n",
    "- Reduce LR: factor = 0.4, patience = 7\n",
    "\n",
    "MÉTRICAS\n",
    "- val_categorical_accuracy = {val_categorical_accuracy}\n",
    "\"\"\"\n",
    "file.write(texto)\n",
    "file.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
