- Despues de que Eric grabara su parte, Andrés encontro un paper que afirmaba nuestra hipótesis de que la forma piramidal de las redes comvolucionales mejora su desempeño .

- El método descrito por Eric es ADMM, Alternating directional multipliers methods o alternating directional method of multipliers.

Sitios de interes:


- [Sobre ADMM](https://core.ac.uk/download/pdf/4380856.pdf)
- [ADMM en redes neuronales](https://dl.acm.org/doi/pdf/10.1145/3292500.3330936)
- [Todas la documentacion de ekras](https://keras.io/api/)
- [Sobre el optimizador Ftrl](https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf)
- [Desempeño de la estructura piramidal en cnn](https://pubmed.ncbi.nlm.nih.gov/28359221/)
- [RMSprop vs all](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)

- [Dropout efectivo y eficiente](https://arxiv.org/pdf/1904.03392.pdf)

- [Tips para deep learning](https://towardsdatascience.com/a-bunch-of-tips-and-tricks-for-training-deep-neural-networks-3ca24c31ddc8)

- [ADAM amsgrad](https://openreview.net/pdf?id=ryQu7f-RZ)

- [Sobre Batch Normalization](https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/)
